{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed patient data\n",
    "df_patientdata = pd.read_csv('data/preprocessed/patient_data.csv', dtype={'stay_id': str, 'subject_id': str, 'acuity': str, 'disposition': str, 'complexity': str, 'los': float})\n",
    "\n",
    "df_patientdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed event logs\n",
    "df_eventlogs = pd.read_csv('data/preprocessed/event_logs.csv', dtype={'case_id': str, 'activity_name': str})\n",
    "df_eventlogs['timestamp'] = pd.to_datetime(df_eventlogs['timestamp'])\n",
    "\n",
    "df_eventlogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of ED activities\n",
    "list_activities = sorted(set(df_eventlogs['activity_name']))\n",
    "\n",
    "list_activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group event logs per case ID\n",
    "df_eventlogs_per_caseid = df_eventlogs.groupby(['case_id']).aggregate({'activity_name': list, 'timestamp': list}).reset_index()\n",
    "\n",
    "df_eventlogs_per_caseid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge patient data with event logs\n",
    "df_ehrs = pd.merge(df_patientdata, df_eventlogs_per_caseid, how='right', left_on='stay_id', right_on='case_id')\n",
    "df_ehrs.dropna(inplace=True)\n",
    "\n",
    "df_ehrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impose a 5-minute temporal resolution\n",
    "TEMPORAL_RESOLUTION = 5  # in minutes\n",
    "new_activity_col, new_timestamp_col = [], []\n",
    "for idx in tqdm(df_ehrs.index):\n",
    "    row = df_ehrs.loc[idx]\n",
    "\n",
    "    activity_list, timestamp_list = row['activity_name'], row['timestamp']\n",
    "\n",
    "    new_activity_list, new_timestamp_list = [activity_list[0]], [timestamp_list[0]]\n",
    "    for act_idx, time_idx in zip(activity_list[1:], timestamp_list[1:]):\n",
    "        if act_idx != new_activity_list[-1]:\n",
    "            new_timestamp_list.append(time_idx)\n",
    "            new_activity_list.append(act_idx)\n",
    "        else:\n",
    "            if (time_idx - new_timestamp_list[-1]).total_seconds() > (60 * TEMPORAL_RESOLUTION):\n",
    "                new_timestamp_list.append(time_idx)\n",
    "                new_activity_list.append(act_idx)\n",
    "\n",
    "    new_timestamp_col.append(new_timestamp_list)\n",
    "    new_activity_col.append(''.join(new_activity_list))\n",
    "\n",
    "df_ehrs.loc[:, 'activity_name'] = new_activity_col\n",
    "df_ehrs.loc[:, 'timestamp'] = np.array(new_timestamp_col, dtype=object)\n",
    "\n",
    "df_ehrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column for retrospective cohort groupname\n",
    "df_ehrs.loc[:, 'groupname'] = df_ehrs.loc[:, 'acuity'] + '-' + df_ehrs.loc[:, 'disposition'] + '-' + df_ehrs.loc[:, 'complexity']\n",
    "\n",
    "# Add column for case length\n",
    "df_ehrs.loc[:, 'case_len'] = df_ehrs.activity_name.str.len()\n",
    "\n",
    "df_ehrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ehrs = df_ehrs[df_ehrs['activity_name'].str.startswith('A')].copy()\n",
    "\n",
    "df_ehrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ehrs = df_ehrs[df_ehrs['activity_name'].str.endswith('G')].copy()\n",
    "\n",
    "df_ehrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ehrs = df_ehrs[df_ehrs['case_len'] >= 3].copy()\n",
    "\n",
    "df_ehrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot percentage of EHRs with the activity\n",
    "x_list, y_list = [], []\n",
    "for act_name in sorted(list_activities):\n",
    "    df_act = df_ehrs[df_ehrs['activity_name'].str.contains(act_name)]\n",
    "    print(act_name, len(df_act)/len(df_ehrs) * 100)\n",
    "    x_list.append(act_name)\n",
    "    y_list.append(round(len(df_act)/len(df_ehrs) * 100, 2))\n",
    "ax = sns.barplot(x=x_list, y=y_list, color='blue')\n",
    "ax.bar_label(ax.containers[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save case length distribution targets\n",
    "outpath_caselen = 'params/caselen'\n",
    "if not os.path.exists(outpath_caselen):\n",
    "    os.makedirs(outpath_caselen)\n",
    "\n",
    "# For entire patient population\n",
    "with open(f'{outpath_caselen}/caselen_overall.txt', 'w+') as filehandle:\n",
    "    json.dump(df_ehrs.case_len.to_list(), filehandle)\n",
    "\n",
    "# For each patient indicator\n",
    "for variable_name in ['acuity', 'disposition', 'complexity']:\n",
    "    for variable_val in set(df_ehrs[variable_name].values):\n",
    "        df_patientgroup = df_ehrs[df_ehrs[variable_name] == variable_val]\n",
    "\n",
    "        patientgroup_caselen = df_patientgroup.case_len.to_list()\n",
    "        with open(f'{outpath_caselen}/caselen_{variable_name}_{variable_val}.txt', 'w+') as filehandle:\n",
    "            json.dump(patientgroup_caselen, filehandle)\n",
    "\n",
    "# For every combination of all patient indicators\n",
    "for variable_val in set(df_ehrs['groupname'].values):\n",
    "    df_patientgroup = df_ehrs[df_ehrs['groupname'] == variable_val]\n",
    "\n",
    "    patientgroup_caselen = df_patientgroup.case_len.to_list()\n",
    "    with open(f'{outpath_caselen}/caselen_groupname_{variable_val}.txt', 'w+') as filehandle:\n",
    "        json.dump(patientgroup_caselen, filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save length of stay distribution targets\n",
    "outpath_los = 'params/los'\n",
    "if not os.path.exists(outpath_los):\n",
    "    os.makedirs(outpath_los)\n",
    "\n",
    "# For entire patient population\n",
    "baseline_los = df_ehrs['los'].to_list()\n",
    "with open(f'{outpath_los}/los_overall.txt', 'w+') as filehandle:\n",
    "    json.dump(baseline_los, filehandle)\n",
    "\n",
    "# For each patient indicator\n",
    "for variable_name in ['acuity', 'disposition', 'complexity']:\n",
    "    for variable_val in set(df_ehrs[variable_name].values):\n",
    "        df_patientgroup = df_ehrs[df_ehrs[variable_name] == variable_val]\n",
    "\n",
    "        patientgroup_los = df_patientgroup['los'].to_list()\n",
    "        with open(f'{outpath_los}/los_{variable_name}_{variable_val}.txt', 'w+') as filehandle:\n",
    "            json.dump(patientgroup_los, filehandle)\n",
    "\n",
    "# For every combination of all patient indicators\n",
    "for variable_val in set(df_ehrs['groupname'].values):\n",
    "    df_patientgroup = df_ehrs[df_ehrs['groupname'] == variable_val]\n",
    "\n",
    "    patientgroup_los = df_patientgroup['los'].to_list()\n",
    "    with open(f'{outpath_los}/los_groupname_{variable_val}.txt', 'w+') as filehandle:\n",
    "        json.dump(patientgroup_los, filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save frequency distributions of patient indicator\n",
    "outpath_frequency = 'params/frequency'\n",
    "if not os.path.exists(outpath_frequency):\n",
    "    os.makedirs(outpath_frequency)\n",
    "\n",
    "# For acuity\n",
    "df_frequency = df_ehrs.groupby(['acuity']).case_id.count() / df_ehrs.case_id.count()\n",
    "df_frequency = df_frequency.rename('percent')\n",
    "df_frequency.to_csv(f'{outpath_frequency}/frequency.csv')\n",
    "\n",
    "# For disposition and complexity per acuity\n",
    "df_frequency = df_ehrs.groupby(['acuity', 'disposition', 'complexity']).case_id.count() / df_ehrs.groupby('acuity').case_id.count()\n",
    "df_frequency = df_frequency.rename('percent')\n",
    "df_frequency.to_csv(f'{outpath_frequency}/frequency_groupname.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save independent branching probabilities for both prospective (acuity) and retrosective (groupname) cohort types\n",
    "outpath_branching = 'params/branching'\n",
    "if not os.path.exists(outpath_branching):\n",
    "    os.makedirs(outpath_branching)\n",
    "\n",
    "for variable_name in tqdm(['groupname', 'acuity']):\n",
    "    for variable_val in tqdm(set(df_ehrs[variable_name].values)):\n",
    "        df_patientgroup = df_ehrs[df_ehrs[variable_name] == variable_val]\n",
    "\n",
    "        dict_edge_count_ind = {}\n",
    "        for edge_from in list_activities:\n",
    "            dict_edge_count_ind[edge_from] = {}\n",
    "            for edge_to in list_activities:\n",
    "                dict_edge_count_ind[edge_from][edge_to] = 0\n",
    "\n",
    "        for case_act in df_patientgroup['activity_name']:\n",
    "            for idx in range(1,len(case_act)):\n",
    "                edge_from = case_act[idx-1]\n",
    "                edge_to = case_act[idx]\n",
    "\n",
    "                dict_edge_count_ind[edge_from][edge_to] += 1\n",
    "\n",
    "        df_edge_count_ind = pd.DataFrame.from_dict(dict_edge_count_ind).T\n",
    "        df_edge_count_ind = df_edge_count_ind.div(df_edge_count_ind.sum(axis=1), axis=0)\n",
    "        df_edge_count_ind = df_edge_count_ind.fillna(0.0)\n",
    "        df_edge_count_ind.index.name = 'name'\n",
    "        df_edge_count_ind.to_csv(f'{outpath_branching}/independent_branching_{variable_name}_{variable_val}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save conditional branching probabilities for both prospective (acuity) and retrosective (groupname) cohort types\n",
    "for variable_name in tqdm(['groupname', 'acuity']):\n",
    "    for variable_val in tqdm(set(df_ehrs[variable_name].values)):\n",
    "        df_patientgroup = df_ehrs[df_ehrs[variable_name] == variable_val]\n",
    "\n",
    "        caselen_max = max(set([x for x in df_patientgroup['activity_name'].str.len()]))\n",
    "\n",
    "        dict_caselen = {}\n",
    "        for caselen_idx in range(1, caselen_max+1):\n",
    "            dict_caselen[caselen_idx] = {}\n",
    "\n",
    "        for case_act in df_patientgroup['activity_name']:\n",
    "            if case_act not in dict_caselen[len(case_act)]:\n",
    "                dict_caselen[len(case_act)][case_act] = {'count': 1, 'prob': 0}\n",
    "            else:\n",
    "                dict_caselen[len(case_act)][case_act]['count'] += 1\n",
    "\n",
    "            for idx in range(len(case_act)-1):\n",
    "                first_part = case_act[:idx+1]\n",
    "                case_len = len(first_part)\n",
    "\n",
    "                if first_part not in dict_caselen[case_len]:\n",
    "                    dict_caselen[case_len][first_part] = {'count': 1, 'prob': 0}\n",
    "                else:\n",
    "                    dict_caselen[case_len][first_part]['count'] += 1\n",
    "\n",
    "        for case_len, possible_cases in sorted(dict_caselen.items()):\n",
    "            for case_idx in possible_cases.keys():\n",
    "                if case_len > 1:\n",
    "                    prior_count = dict_caselen[case_len-1][case_idx[:case_len-1]]['count']\n",
    "                    prior_prob = dict_caselen[case_len-1][case_idx[:case_len-1]]['prob']\n",
    "                    possible_cases[case_idx]['prob'] = (possible_cases[case_idx]['count'] / len(df_ehrs)) / (prior_count / len(df_ehrs))\n",
    "                else:\n",
    "                    possible_cases[case_idx]['prob'] = possible_cases[case_idx]['count'] / len(df_ehrs)\n",
    "\n",
    "        dict_edge_count_con = {}\n",
    "        for caselen_idx in dict_caselen.keys():\n",
    "            if caselen_idx != 1:\n",
    "                for case_idx in sorted(dict_caselen[caselen_idx]):\n",
    "                    prefix_name = case_idx[:-1]\n",
    "                    final_char = case_idx[-1]\n",
    "                    if prefix_name not in dict_edge_count_con.keys():\n",
    "                        dict_edge_count_con[prefix_name] = {value: 0 for value in list_activities}\n",
    "                    dict_edge_count_con[prefix_name][final_char] = dict_caselen[caselen_idx][case_idx]['prob']\n",
    "\n",
    "        df_edge_count_con = pd.DataFrame.from_dict(dict_edge_count_con, orient='index')\n",
    "        df_edge_count_con.index.name = 'name'\n",
    "        df_edge_count_con.to_csv(f'{outpath_branching}/conditional_branching_{variable_name}_{variable_val}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify records with temporal deviations\n",
    "TEMPORAL_DEVIATION = 3  # z-score\n",
    "\n",
    "dict_edge_duration = {}\n",
    "for edge_from in list_activities:\n",
    "    for edge_to in list_activities:\n",
    "        dict_edge_duration[(edge_from, edge_to)] = []\n",
    "\n",
    "for case_time, case_act in zip(df_ehrs['timestamp'], df_ehrs['activity_name']):\n",
    "    assert len(case_time) == len(case_act)\n",
    "\n",
    "    for idx in range(1, len(case_time)):\n",
    "        edge_name = (case_act[idx-1], case_act[idx])\n",
    "        edge_duration = (case_time[idx] - case_time[idx-1]).total_seconds() / (60*60)\n",
    "\n",
    "        dict_edge_duration[edge_name].append(edge_duration)\n",
    "\n",
    "dict_edge_stats = {}\n",
    "for edge_name, list_edge_durations in dict_edge_duration.items():\n",
    "    if len(list_edge_durations) != 0:\n",
    "        dict_edge_stats[edge_name] = {'mean': np.mean(list_edge_durations),\n",
    "                                      'std': np.std(list_edge_durations)\n",
    "                                      }\n",
    "\n",
    "        if np.std(list_edge_durations) == 0:\n",
    "            dict_edge_stats[edge_name]['std'] = 1e-10  # To prevent divide by 0\n",
    "\n",
    "    else:\n",
    "        dict_edge_stats[edge_name] = {'mean': 0.,\n",
    "                                      'std': 1e-10\n",
    "                                      }\n",
    "\n",
    "list_deviation_count = []\n",
    "for case_time, case_act in zip(df_ehrs['timestamp'], df_ehrs['activity_name']):\n",
    "    assert len(case_time) == len(case_act)\n",
    "\n",
    "    if TEMPORAL_DEVIATION != 0:\n",
    "        case_deviation_count = 0\n",
    "        for idx in range(1,len(case_time)):\n",
    "            edge_name = (case_act[idx-1], case_act[idx])\n",
    "            edge_duration = (case_time[idx] - case_time[idx-1]).total_seconds() / (60*60)\n",
    "\n",
    "            if abs((edge_duration - dict_edge_stats[edge_name]['mean']) / dict_edge_stats[edge_name]['std']) >= TEMPORAL_DEVIATION:\n",
    "                case_deviation_count += 1\n",
    "\n",
    "        list_deviation_count.append(case_deviation_count)\n",
    "    else:\n",
    "        list_deviation_count.append(0)\n",
    "\n",
    "df_ehrs.loc[:, 'temporal_deviation_count'] = list_deviation_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save execution time distributions for prospective and retrospective cohort types\n",
    "outpath_time = f'params/time_std{TEMPORAL_DEVIATION}'\n",
    "if not os.path.exists(outpath_time):\n",
    "    os.makedirs(outpath_time)\n",
    "\n",
    "for variable_name in tqdm(['groupname', 'acuity']):\n",
    "    for variable_val in tqdm(set(df_ehrs[variable_name].values)):\n",
    "        df_patientgroup = df_ehrs[df_ehrs[variable_name] == variable_val]\n",
    "\n",
    "        df_patientgroup = df_patientgroup[df_patientgroup['temporal_deviation_count'] == 0]\n",
    "\n",
    "        dict_edge_duration = {}\n",
    "        for edge_from in list_activities:\n",
    "            for edge_to in list_activities:\n",
    "                dict_edge_duration[(edge_from, edge_to)] = []\n",
    "\n",
    "        for case_time, case_act in zip(df_patientgroup['timestamp'], df_patientgroup['activity_name']):\n",
    "            assert len(case_time) == len(case_act)\n",
    "\n",
    "            for idx in range(1, len(case_time)):\n",
    "                edge_name = (case_act[idx-1], case_act[idx])\n",
    "                edge_duration = (case_time[idx] - case_time[idx-1]).total_seconds() / (60*60)\n",
    "\n",
    "                dict_edge_duration[edge_name].append(edge_duration)\n",
    "\n",
    "        list_edge_names = []\n",
    "        list_edge_time_distribution = []\n",
    "        for edge_name, list_edge_durations in dict_edge_duration.items():\n",
    "            list_edge_names.append(edge_name[0] + edge_name[1])\n",
    "\n",
    "            if len(list_edge_durations) != 0:\n",
    "                dict_duration_count = {}\n",
    "                all_count = 0\n",
    "                for duration_idx in list_edge_durations:\n",
    "                    if duration_idx not in dict_duration_count.keys():\n",
    "                        dict_duration_count[duration_idx] = 1\n",
    "                    else:\n",
    "                        dict_duration_count[duration_idx] += 1\n",
    "                    all_count += 1\n",
    "\n",
    "                dict_duration_count_relative = {key: value/all_count for key, value in dict_duration_count.items()}\n",
    "                list_edge_time_distribution.append(dict_duration_count_relative)\n",
    "\n",
    "            else:\n",
    "                list_edge_time_distribution.append(0)\n",
    "\n",
    "        df_pairwise = pd.DataFrame({'edge': list_edge_names, 'param': list_edge_time_distribution})\n",
    "        df_pairwise.to_csv(f'{outpath_time}/time_{variable_name}_{variable_val}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
